# 深度强化学习面试题目
1. 什么是强化学习？
> 强化学习（Reinforcement Learning, RL），又称增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题
![](assets/markdown-img-paste-20190923214243777.png)


2. 强化学习和监督学习、无监督学习的区别是什么？
3. 强化学习适合解决什么样子的问题？
4. 强化学习的损失函数（loss function）是什么？和深度学习的损失函数有何关系？
5. POMDP是什么？马尔科夫过程是什么？马尔科夫决策过程是什么？里面的“马尔科夫”体现了什么性质？
6. 贝尔曼方程的具体数学表达式是什么？
7. 最优值函数和最优策略为什么等价？
8.  为什么最优值函数就等同最优策略
9. 如果不满足马尔科夫性怎么办？当前时刻的状态和它之前很多很多个状态都有关之间关系？
10. 求解马尔科夫决策过程都有哪些方法？有模型用什么方法？动态规划是怎么回事？
11. 简述动态规划(DP)算法？
12. 简述蒙特卡罗估计值函数(MC)算法。
13. 简述时间差分(TD)算法。
14. 简述动态规划、蒙特卡洛和时间差分的对比（共同点和不同点）
15. MC和TD分别是无偏估计吗？
16. MC、TD谁的方差大，为什么？
17. 简述on-policy和off-policy的区别
18. 简述Q-Learning，写出其Q(s,a)更新公式。它是on-policy还是off-policy，为什么？
19. 写出用第n步的值函数更新当前值函数的公式（1-step，2-step，n-step的意思）。当n的取值变大时，期望和方差分别变大、变小？
20. TD（λ）方法：当λ=0时实际上与哪种方法等价，λ=1呢？
21. 写出蒙特卡洛、TD和TD（λ）这三种方法更新值函数的公式？
22. value-based和policy-based的区别是什么？
23. DQN的两个关键trick分别是什么？
24. 阐述目标网络和experience replay的作用？
25. 手工推导策略梯度过程？
26. 描述随机策略和确定性策略的特点？
27. 不打破数据相关性，神经网络的训练效果为什么就不好？
28. 画出DQN玩Flappy Bird的流程图。在这个游戏中，状态是什么，状态是怎么转移的？奖赏函数如何设计，有没有奖赏延迟问题？
29. DQN都有哪些变种？引入状态奖励的是哪种？
30. 简述double DQN原理？
31. 策略梯度方法中基线baseline如何确定？
32. 画出DDPG框架结构图？
33. Actor-Critic两者的区别是什么？
34. actor-critic框架中的critic起了什么作用？
35. DDPG是on-policy还是off-policy，为什么？
36. 是否了解过D4PG算法？简述其过程
37. 简述A3C算法？A3C是on-policy还是off-policy，为什么？
38. A3C算法是如何异步更新的？是否能够阐述GA3C和A3C的区别？
39. 简述A3C的优势函数？
40. 什么是重要性采样？
41. 为什么TRPO能保证新策略的回报函数单调不减？
42. TRPO是如何通过优化方法使每个局部点找到让损失函数非增的最优步长来解决学习率的问题；
43. 如何理解利用平均KL散度代替最大KL散度？
44. 简述PPO算法？与TRPO算法有何关系？
45. 简述DPPO和PPO的关系？
46. 强化学习如何用在推荐系统中？
47. 推荐场景中奖赏函数如何设计？
48. 场景中状态是什么，当前状态怎么转移到下一状态？
49. 自动驾驶和机器人的场景如何建模成强化学习问题？MDP各元素对应真实场景中的哪些变量？
50. 强化学习需要大量数据，如何生成或采集到这些数据？
51. 是否用某种DRL算法玩过Torcs游戏？具体怎么解决？
52. 是否了解过奖励函数的设置(reward shaping)？






参考及引用链接：
[1]https://zhuanlan.zhihu.com/p/33133828
[2]https://aemah.github.io/2018/11/07/RL_interview/
